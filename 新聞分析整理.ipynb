{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=UTF-8\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "jieba.set_dictionary('dict.txt.big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"result.json\", 'r', encoding='utf-8')\n",
    "f = file.read()\n",
    "jsondata = json.loads(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618\n"
     ]
    }
   ],
   "source": [
    "count1 = 0\n",
    "\n",
    "list_society = []\n",
    "list_life = []\n",
    "list_entertainment = []\n",
    "list_international = []\n",
    "list_headline = []\n",
    "list_sport = []\n",
    "list_political = []\n",
    "list_animal = []\n",
    "list_finance = []\n",
    "list_3c = []\n",
    "list_fashion = []\n",
    "list_forum = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "url_society = []\n",
    "url_life = []\n",
    "url_entertainment = []\n",
    "url_international = []\n",
    "url_headline = []\n",
    "url_sport = []\n",
    "url_political = []\n",
    "url_animal = []\n",
    "url_finance = []\n",
    "url_3c = []\n",
    "url_fashion = []\n",
    "url_forum = []\n",
    "\n",
    "view_society = []\n",
    "view_life = []\n",
    "view_entertainment = []\n",
    "view_international = []\n",
    "view_headline = []\n",
    "view_sport = []\n",
    "view_political = []\n",
    "view_animal = []\n",
    "view_finance = []\n",
    "view_3c = []\n",
    "view_fashion = []\n",
    "view_forum = []\n",
    "\n",
    "\n",
    "for category in jsondata:\n",
    "    if '\\\"keywords\\\": [\\\"社會\\\"]' in category['category'][0]:\n",
    "        list_society.append(category['title'][0])\n",
    "        url_society.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_society.append(0)\n",
    "        else:\n",
    "            view_society.append(int(category['view'][0]))\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"生活\\\"]' in category['category'][0]:\n",
    "        list_life.append(category['title'][0])\n",
    "        url_life.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_life.append(0)\n",
    "        else:\n",
    "            view_life.append(category['view'][0])\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"娛樂\\\"]' in category['category'][0]:\n",
    "        list_entertainment.append(category['title'][0])\n",
    "        url_entertainment.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_entertainment.append(0)\n",
    "        else:\n",
    "            view_entertainment.append(category['view'][0])\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"國際\\\"]' in category['category'][0]:\n",
    "        list_international.append(category['title'][0])\n",
    "        url_international.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_international.append(0)\n",
    "        else:\n",
    "            view_international.append(category['view'][0])\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"頭條要聞\\\"]' in category['category'][0]:\n",
    "        list_headline.append(category['title'][0])\n",
    "        url_headline.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_headline.append(0)\n",
    "        else:\n",
    "            view_headline.append(category['view'][0])\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"體育\\\"]' in category['category'][0]:\n",
    "        list_sport.append(category['title'][0])\n",
    "        url_sport.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_sport.append(0)\n",
    "        else:\n",
    "            view_sport.append(category['view'][0])\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"政治\\\"]' in category['category'][0]:\n",
    "        list_political.append(category['title'][0])\n",
    "        url_political.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_political.append(0)\n",
    "        else:\n",
    "            view_political.append(category['view'][0])\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"動物\\\"]' in category['category'][0]:\n",
    "        list_animal.append(category['title'][0])\n",
    "        url_animal.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_animal.append(0)\n",
    "        else:\n",
    "            view_animal.append(category['view'][0])\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"財經\\\"]' in category['category'][0]:\n",
    "        list_finance.append(category['title'][0])\n",
    "        url_finance.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_finance.append(0)\n",
    "        else:\n",
    "            view_finance.append(category['view'][0])\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"3C\\\"]' in category['category'][0]:\n",
    "        list_3c.append(category['title'][0])\n",
    "        url_3c.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_3c.append(0)\n",
    "        else:\n",
    "            view_3c.append(category['view'][0])\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"時尚\\\"]' in category['category'][0]:\n",
    "        list_fashion.append(category['title'][0])\n",
    "        url_fashion.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_fashion.append(0)\n",
    "        else:\n",
    "            view_fashion.append(category['view'][0])\n",
    "        \n",
    "    elif '\\\"keywords\\\": [\\\"論壇\\\"]' in category['category'][0]:\n",
    "        list_forum.append(category['title'][0])\n",
    "        url_forum.append(category['url'])\n",
    "        if category['view'] == []:\n",
    "            view_forum.append(0)\n",
    "        else:\n",
    "            view_forum.append(category['view'][0])\n",
    "        \n",
    "    count1 = count1 + 1    \n",
    "print(count1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_society = ''\n",
    "title_life = ''\n",
    "title_entertainment = ''\n",
    "title_international = ''\n",
    "title_headline = ''\n",
    "title_sport = ''\n",
    "title_political = ''\n",
    "title_animal = ''\n",
    "title_finance = ''\n",
    "title_3c = ''\n",
    "title_fashion = ''\n",
    "title_forum = ''\n",
    "\n",
    "\n",
    "for title in list_society:\n",
    "    title_society = title_society + title \n",
    "    \n",
    "for title in list_life:\n",
    "    title_life = title_life + title\n",
    "    \n",
    "    \n",
    "####################  entertainment  ############################\n",
    "\n",
    "for title in list_entertainment:\n",
    "    title_entertainment = title_entertainment + title\n",
    "\n",
    "####################  international  ############################\n",
    "\n",
    "    \n",
    "for title in list_international:\n",
    "    title_international = title_international + title\n",
    "\n",
    "####################  headline  ############################\n",
    "\n",
    "    \n",
    "for title in list_headline:\n",
    "    title_headline = title_headline + title\n",
    "    \n",
    "####################  sport  ############################\n",
    "\n",
    "    \n",
    "for title in list_sport:\n",
    "    title_sport = title_sport + title\n",
    "    \n",
    "    \n",
    "####################  political  ############################\n",
    "\n",
    "    \n",
    "for title in list_political:\n",
    "    title_political = title_political + title\n",
    "    \n",
    "####################  animal  ############################\n",
    "\n",
    "    \n",
    "for title in list_animal:\n",
    "    title_animal = title_animal + title\n",
    "    \n",
    "####################  finance  ############################\n",
    "\n",
    "for title in list_finance:\n",
    "    title_finance = title_finance + title\n",
    "    \n",
    "####################  3c  ############################\n",
    "\n",
    "    \n",
    "for title in list_3c:\n",
    "    title_3c = title_3c + title\n",
    "    \n",
    "####################  fashion  ############################\n",
    "\n",
    "    \n",
    "for title in list_fashion:\n",
    "    title_fashion = title_fashion + title\n",
    "    \n",
    "####################  forum  ############################\n",
    "\n",
    "    \n",
    "for title in list_forum:\n",
    "    title_forum = title_forum + title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\user\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.ua7677c818aa16337f448e65eab47ca3f.cache\n",
      "Loading model cost 214.018 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.analyse.set_stop_words(\"stop_words.txt\")\n",
    "jieba.load_userdict('繁體中文詞庫.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 391, 611, 349, 353]\n",
      "[369757, 301327, 282863, 281541, 279441]\n"
     ]
    }
   ],
   "source": [
    "view_list = []\n",
    "top5_view = [0,0,0,0,0]\n",
    "top5_view_where = [0,0,0,0,0]\n",
    "title_list = []\n",
    "url_list = []\n",
    "count3 = 0\n",
    "for view in jsondata:\n",
    "    count3 = count3 + 1\n",
    "    if view[\"view\"] == []:\n",
    "        view_list.append(0)\n",
    "    else:\n",
    "        view_list.append(int(view[\"view\"][0]))\n",
    "    title_list.append(view[\"title\"][0])\n",
    "    url_list.append(view[\"url\"])\n",
    "for j in range(0,5,1):\n",
    "    for i in range(0,len(view_list),1):\n",
    "        if top5_view[j] < view_list[i]:\n",
    "            top5_view[j] = view_list[i]\n",
    "            top5_view_where[j] = i+j\n",
    "    view_list.pop(top5_view_where[j]-j) \n",
    "print(top5_view_where) \n",
    "print(top5_view)\n",
    "title_list_5 = []\n",
    "url_list_5 = []\n",
    "for k in top5_view_where:\n",
    "    \n",
    "    title_list_5.append(title_list[k])\n",
    "    url_list_5.append(url_list[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "top_news_dict = {\n",
    "        'title' : title_list_5,\n",
    "        'url' :url_list_5,\n",
    "        'view': top5_view\n",
    "    }\n",
    "file2 = open(\"top_news_all.json\",'w', encoding='utf-8')\n",
    "json.dump(top_news_dict ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('立院', 0.16860660224797844), ('勞團', 0.11278082549905659), ('時力', 0.08055773249932613), ('靜坐', 0.08055773249932613), ('驅離', 0.0644461859994609)]\n"
     ]
    }
   ],
   "source": [
    "tags_society = jieba.analyse.extract_tags(title_society, 5,True)\n",
    "print(tags_society)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.97274669286793\n",
      "126.45495168598383\n",
      "44.34353639121833\n",
      "172.48455409968193\n",
      "506.8314463574232\n",
      "437.70273943575205\n",
      "99.14068212181132\n",
      "117.51880176684097\n",
      "3863.957503716922\n",
      "69.13464603092169\n",
      "104.99894853962168\n",
      "84.58561912429244\n",
      "29.661357106251884\n",
      "115.37478448553489\n",
      "118.64542842500754\n",
      "292.7790229955509\n",
      "66.31512539344527\n",
      "78.60823537284244\n",
      "2584.5981779618796\n",
      "684.4990530467741\n",
      "146.1317267537776\n",
      "242.15654389297436\n",
      "329.72279911974186\n",
      "269.7072884077439\n",
      "138.7204153638396\n",
      "472.5516588410471\n",
      "146.1317267537776\n",
      "242.15654389297436\n",
      "277.4408307276792\n",
      "329.72279911974186\n",
      "269.7072884077439\n",
      "116.90538140302208\n",
      "193.72523511437947\n",
      "221.95266458214337\n",
      "110.97633229107169\n"
     ]
    }
   ],
   "source": [
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_society_2 = []\n",
    "url_society_2 = []\n",
    "\n",
    "\n",
    "for keyword in tags_society:\n",
    "    for i in range(0,len(list_society)):\n",
    "        if keyword[0] in list_society[i]:\n",
    "            print(float(view_society[i])*keyword[1])\n",
    "            point_news.append(float(view_society[i])*keyword[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_society = jieba.analyse.extract_tags(title_society, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "list_society_2 = []\n",
    "url_society_2 = []\n",
    "list_society_3 = []\n",
    "url_society_3 = []\n",
    "\n",
    "for keyword in tags_society:\n",
    "    for i in range(0,len(list_society)):\n",
    "        if keyword[0] in list_society[i]:\n",
    "            \n",
    "            point_news.append(float(view_society[i])*keyword[1])\n",
    "            list_society_3.append(list_society[i])\n",
    "            url_society_3.append(url_society[i])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "          \n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "\n",
    "\n",
    "    \n",
    "i = 0\n",
    "json__society = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_society_3 == []:\n",
    "        break\n",
    "    if list_society_3[point_news_where[i]] in list_society_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_society_2.append( list_society_3[point_news_where[i]])\n",
    "        url_society_2.append( url_society_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_society= {\n",
    "    \"title\":list_society_2,\n",
    "    \"url\" : url_society_2\n",
    "}\n",
    "json__society = json__society + str(top5_society)\n",
    "        \n",
    "        \n",
    "json__society = json__society + '));'\n",
    "\n",
    "file2 = open(\"news_society.json\",'w', encoding='utf-8')\n",
    "json.dump(json__society ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_life = jieba.analyse.extract_tags(title_life, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_life_2 = []\n",
    "url_life_2 = []\n",
    "list_life_3 = []\n",
    "url_life_3 = []\n",
    "\n",
    "for keyword in tags_life:\n",
    "    for i in range(0,len(list_life)):\n",
    "        if keyword[0] in list_life[i]:\n",
    "            \n",
    "            point_news.append(float(view_life[i])*keyword[1])\n",
    "            list_life_3.append(list_life[i])\n",
    "            url_life_3.append(url_life[i])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "         \n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "i = 0\n",
    "json__life = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_life_3 == []:\n",
    "        break\n",
    "    if list_life_3[point_news_where[i]] in list_life_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_life_2.append( list_life_3[point_news_where[i]])\n",
    "        url_life_2.append( url_life_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_life= {\n",
    "    \"title\":list_life_2,\n",
    "    \"url\" : url_life_2\n",
    "}\n",
    "json__life = json__life + str(top5_life)\n",
    "        \n",
    "        \n",
    "json__life = json__life + '));'\n",
    "\n",
    "file2 = open(\"news_life.json\",'w', encoding='utf-8')\n",
    "json.dump(json__life ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_entertainment = jieba.analyse.extract_tags(title_entertainment, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_entertainment_2 = []\n",
    "url_entertainment_2 = []\n",
    "list_entertainment_3 = []\n",
    "url_entertainment_3 = []\n",
    "\n",
    "for keyword in tags_entertainment:\n",
    "    for i in range(0,len(list_entertainment)):\n",
    "        if keyword[0] in list_entertainment[i]:\n",
    "            \n",
    "            point_news.append(float(view_entertainment[i])*keyword[1])\n",
    "            list_entertainment_3.append(list_entertainment[i])\n",
    "            url_entertainment_3.append(url_entertainment[i])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "         \n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "    \n",
    "i = 0\n",
    "json__entertainment = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_entertainment_3 == []:\n",
    "        break\n",
    "    if list_entertainment_3[point_news_where[i]] in list_entertainment_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_entertainment_2.append( list_entertainment_3[point_news_where[i]])\n",
    "        url_entertainment_2.append( url_entertainment_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_entertainment= {\n",
    "    \"title\":list_entertainment_2,\n",
    "    \"url\" : url_entertainment_2\n",
    "}\n",
    "json__entertainment = json__entertainment + str(top5_entertainment)\n",
    "        \n",
    "        \n",
    "json__entertainment = json__entertainment + '));'\n",
    "\n",
    "file2 = open(\"news_entertainment.json\",'w', encoding='utf-8')\n",
    "json.dump(json__entertainment ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_international = jieba.analyse.extract_tags(title_international, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_international_2 = []\n",
    "url_international_2 = []\n",
    "list_international_3 = []\n",
    "url_international_3 = []\n",
    "\n",
    "for keyword in tags_international:\n",
    "    for i in range(0,len(list_international)):\n",
    "        if keyword[0] in list_international[i]:\n",
    "            list_international_3.append(list_international[i])\n",
    "            url_international_3.append(url_international[i])\n",
    "            point_news.append(float(view_international[i])*keyword[1])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "         \n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "    \n",
    "i = 0\n",
    "json__international = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_international_3 == []:\n",
    "        break\n",
    "    if list_international_3[point_news_where[i]] in list_international_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_international_2.append( list_international_3[point_news_where[i]])\n",
    "        url_international_2.append( url_international_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_international= {\n",
    "    \"title\":list_international_2,\n",
    "    \"url\" : url_international_2\n",
    "}\n",
    "json__international = json__international + str(top5_international)\n",
    "        \n",
    "        \n",
    "json__international = json__international + '));'\n",
    "\n",
    "file2 = open(\"news_international.json\",'w', encoding='utf-8')\n",
    "json.dump(json__international ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_headline = jieba.analyse.extract_tags(title_headline, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_headline_2 = []\n",
    "url_headline_2 = []\n",
    "list_headline_3 = []\n",
    "url_headline_3 = []\n",
    "\n",
    "for keyword in tags_headline:\n",
    "    for i in range(0,len(list_headline)):\n",
    "        if keyword[0] in list_headline[i]:\n",
    "            list_headline_3.append(list_headline[i])\n",
    "            url_headline_3.append(url_headline[i])\n",
    "            point_news.append(float(view_headline[i])*keyword[1])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "    \n",
    "i = 0\n",
    "json__headline = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_headline_3 == []:\n",
    "        break\n",
    "    if list_headline_3[point_news_where[i]] in list_headline_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_headline_2.append( list_headline_3[point_news_where[i]])\n",
    "        url_headline_2.append( url_headline_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_headline= {\n",
    "    \"title\":list_headline_2,\n",
    "    \"url\" : url_headline_2\n",
    "}\n",
    "json__headline = json__headline + str(top5_headline)\n",
    "        \n",
    "        \n",
    "json__headline = json__headline + '));'\n",
    "\n",
    "file2 = open(\"news_headline.json\",'w', encoding='utf-8')\n",
    "json.dump(json__headline ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_sport = jieba.analyse.extract_tags(title_sport, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_sport_2 = []\n",
    "url_sport_2 = []\n",
    "list_sport_3 = []\n",
    "url_sport_3 = []\n",
    "\n",
    "for keyword in tags_sport:\n",
    "    for i in range(0,len(list_sport)):\n",
    "        if keyword[0] in list_sport[i]:\n",
    "            list_sport_3.append(list_sport[i])\n",
    "            url_sport_3.append(url_sport[i])\n",
    "            point_news.append(float(view_sport[i])*keyword[1])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "i = 0\n",
    "json__sport = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_sport_3 == []:\n",
    "        break\n",
    "    if list_sport_3[point_news_where[i]] in list_sport_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_sport_2.append( list_sport_3[point_news_where[i]])\n",
    "        url_sport_2.append( url_sport_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_sport= {\n",
    "    \"title\":list_sport_2,\n",
    "    \"url\" : url_sport_2\n",
    "}\n",
    "json__sport = json__sport + str(top5_sport)\n",
    "        \n",
    "        \n",
    "json__sport = json__sport + '));'\n",
    "\n",
    "file2 = open(\"news_sport.json\",'w', encoding='utf-8')\n",
    "json.dump(json__sport ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_political = jieba.analyse.extract_tags(title_political, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_political_2 = []\n",
    "url_political_2 = []\n",
    "list_political_3 = []\n",
    "url_political_3 = []\n",
    "\n",
    "for keyword in tags_political:\n",
    "    for i in range(0,len(list_political)):\n",
    "        if keyword[0] in list_political[i]:\n",
    "            list_political_3.append(list_political[i])\n",
    "            url_political_3.append(url_political[i])\n",
    "            point_news.append(float(view_political[i])*keyword[1])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "i = 0\n",
    "json__political = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_political_3 == []:\n",
    "        break\n",
    "    if list_political_3[point_news_where[i]] in list_political_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_political_2.append( list_political_3[point_news_where[i]])\n",
    "        url_political_2.append( url_political_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_political= {\n",
    "    \"title\":list_political_2,\n",
    "    \"url\" : url_political_2\n",
    "}\n",
    "json__political = json__political + str(top5_political)\n",
    "        \n",
    "        \n",
    "json__political = json__political + '));'\n",
    "\n",
    "file2 = open(\"news_political.json\",'w', encoding='utf-8')\n",
    "json.dump(json__political ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_animal = jieba.analyse.extract_tags(title_animal, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_animal_2 = []\n",
    "url_animal_2 = []\n",
    "list_animal_3 = []\n",
    "url_animal_3 = []\n",
    "\n",
    "for keyword in tags_animal:\n",
    "    for i in range(0,len(list_animal)):\n",
    "        if keyword[0] in list_animal[i]:\n",
    "            list_animal_3.append(list_animal[i])\n",
    "            url_animal_3.append(url_animal[i])\n",
    "            point_news.append(float(view_animal[i])*keyword[1])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "i = 0\n",
    "json__animal = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_animal_3 == []:\n",
    "        break\n",
    "    if list_animal_3[point_news_where[i]] in list_animal_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_animal_2.append( list_animal_3[point_news_where[i]])\n",
    "        url_animal_2.append( url_animal_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_animal= {\n",
    "    \"title\":list_animal_2,\n",
    "    \"url\" : url_animal_2\n",
    "}\n",
    "json__animal = json__animal + str(top5_animal)\n",
    "        \n",
    "        \n",
    "json__animal = json__animal + '));'\n",
    "\n",
    "file2 = open(\"news_animal.json\",'w', encoding='utf-8')\n",
    "json.dump(json__animal ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_finance = jieba.analyse.extract_tags(title_finance, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_finance_2 = []\n",
    "url_finance_2 = []\n",
    "list_finance_3 = []\n",
    "url_finance_3 = []\n",
    "\n",
    "for keyword in tags_finance:\n",
    "    for i in range(0,len(list_finance)):\n",
    "        if keyword[0] in list_finance[i]:\n",
    "            list_finance_3.append(list_finance[i])\n",
    "            url_finance_3.append(url_finance[i])\n",
    "            point_news.append(float(view_finance[i])*keyword[1])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "i = 0\n",
    "json__finance = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_finance_3 == []:\n",
    "        break\n",
    "    if list_finance_3[point_news_where[i]] in list_finance_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_finance_2.append( list_finance_3[point_news_where[i]])\n",
    "        url_finance_2.append( url_finance_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_finance = {\n",
    "    \"title\":list_finance_2,\n",
    "    \"url\" : url_finance_2\n",
    "}\n",
    "json__finance = json__finance + str(top5_finance)\n",
    "        \n",
    "        \n",
    "json__finance = json__finance + '));'\n",
    "\n",
    "file2 = open(\"news_finance.json\",'w', encoding='utf-8')\n",
    "json.dump(json__finance ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_3c = jieba.analyse.extract_tags(title_3c, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_3c_2 = []\n",
    "url_3c_2 = []\n",
    "list_3c_3 = []\n",
    "url_3c_3 = []\n",
    "\n",
    "for keyword in tags_3c:\n",
    "    for i in range(0,len(list_3c)):\n",
    "        if keyword[0] in list_3c[i]:\n",
    "            list_3c_3.append(list_3c[i])\n",
    "            url_3c_3.append(url_3c[i])\n",
    "            point_news.append(float(view_3c[i])*keyword[1])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "i = 0\n",
    "json__3c = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_3c_3 == []:\n",
    "        break\n",
    "    if list_3c_3[point_news_where[i]] in list_3c_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_3c_2.append( list_3c_3[point_news_where[i]])\n",
    "        url_3c_2.append( url_3c_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_3c = {\n",
    "    \"title\":list_3c_2,\n",
    "    \"url\" : url_3c_2\n",
    "}\n",
    "json__3c = json__3c + str(top5_3c)\n",
    "        \n",
    "        \n",
    "json__3c = json__3c + '));'\n",
    "\n",
    "file2 = open(\"news_3c.json\",'w', encoding='utf-8')\n",
    "json.dump(json__3c ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_fashion = jieba.analyse.extract_tags(title_fashion, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_fashion_2 = []\n",
    "url_fashion_2 = []\n",
    "list_fashion_3 = []\n",
    "url_fashion_3 = []\n",
    "\n",
    "for keyword in tags_fashion:\n",
    "    for i in range(0,len(list_fashion)):\n",
    "        if keyword[0] in list_fashion[i]:\n",
    "            list_fashion_3.append(list_fashion[i])\n",
    "            url_fashion_3.append(url_fashion[i])\n",
    "            point_news.append(float(view_fashion[i])*keyword[1])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "    for i in range(0,len(point_news),1):\n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "i = 0\n",
    "json__fashion = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_fashion_3 == []:\n",
    "        break\n",
    "    if list_fashion_3[point_news_where[i]] in list_fashion_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_fashion_2.append( list_fashion_3[point_news_where[i]])\n",
    "        url_fashion_2.append( url_fashion_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_fashion = {\n",
    "    \"title\":list_fashion_2,\n",
    "    \"url\" : url_fashion_2\n",
    "}\n",
    "json__fashion = json__fashion + str(top5_fashion)\n",
    "        \n",
    "        \n",
    "json__fashion = json__fashion + '));'\n",
    "\n",
    "file2 = open(\"news_fashion.json\",'w', encoding='utf-8')\n",
    "json.dump(json__fashion ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_forum = jieba.analyse.extract_tags(title_forum, 5,True)\n",
    "\n",
    "point_news = []\n",
    "point_news_number = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "point_news_where = [0,0,0,0,0,0,0,0,0,0]\n",
    "list_forum_2 = []\n",
    "url_forum_2 = []\n",
    "list_forum_3 = []\n",
    "url_forum_3 = []\n",
    "\n",
    "for keyword in tags_forum:\n",
    "    for i in range(0,len(list_forum)):\n",
    "        if keyword[0] in list_forum[i]:\n",
    "            list_forum_3.append(list_forum[i])\n",
    "            url_forum_3.append(url_forum[i])\n",
    "            point_news.append(float(view_forum[i])*keyword[1])\n",
    "\n",
    "for j in range(0,len(point_news),1):\n",
    "    if j>9:\n",
    "        break\n",
    "        \n",
    "    for i in range(0,len(point_news),1):\n",
    "        if point_news_number[j] < point_news[i] :\n",
    "            point_news_number[j] = point_news[i]\n",
    "            point_news_where[j] = i\n",
    "            \n",
    "    point_news[point_news_where[j]] = -1\n",
    "    \n",
    "i = 0\n",
    "json__forum = 'callBackdata(JSON.stringify('\n",
    "while i<5:\n",
    "    if list_forum_3 == []:\n",
    "        break\n",
    "    if list_forum_3[point_news_where[i]] in list_forum_2 :\n",
    "        point_news_where.pop(i)\n",
    "        continue\n",
    "    else:\n",
    "        list_forum_2.append( list_forum_3[point_news_where[i]])\n",
    "        url_forum_2.append( url_forum_3[point_news_where[i]])\n",
    "        i = i + 1\n",
    "\n",
    "top5_forum = {\n",
    "    \"title\":list_forum_2,\n",
    "    \"url\" : url_forum_2\n",
    "}\n",
    "json__forum = json__forum + str(top5_forum)\n",
    "        \n",
    "        \n",
    "json__forum = json__forum + '));'\n",
    "\n",
    "file2 = open(\"news_forum.json\",'w', encoding='utf-8')\n",
    "json.dump(json__forum ,file2,indent = 5,sort_keys=True,ensure_ascii=False)  \n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
